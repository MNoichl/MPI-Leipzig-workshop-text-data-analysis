---
format: 
    revealjs:
        revealjs-url: reveal.js-5.2.1
        title-slide: false
        scroll: true
        pagetitle: "Data-Driven methods"
        theme: [default, custom.scss]
        navigation-mode: vertical
        revealjs-plugins: [multimodal]
        include-after-body:
        - file: includes/after-body.html
        multimodal:
           # overlaycolor: "rgba(0,0,0,.55)"
            overlaycolor: "rgb(255, 0, 0)"
            radius: "0"             
        include-in-header:
        - file: includes/in-header.html
        auto-stretch: false
        incremental: true 
        
bibliography: my_lib.bib
# csl: the-journal-of-modern-history.csl
cite-method: citeproc
dependencies:
    - "custom.scss"
controls: true
width: 1400 # 1060
height: 1000 # 700
margin: 0.15 # 1
min-scale: 0.2
max-scale: 1.6

# background-image: "images/background_egg.png"
#quarto preview /Users/Noich001/Desktop/network_epistemology_playground/talk_BOCHUM/Data_and_ABMs.qmd
#quarto add martinomagnifico/quarto-multimodal
---

# {background-iframe="background_network.html" background-size="120%" background-position="left bottom" background-opacity="1."}
::: {.absolute top=-50 left=700 background="#000" .title-block}
<h1 class="title">Social science \n in (latent) \n space!</h1>
<p class="author"> Max Noichl</p>
<p class="date">2025-09-17</p>
:::



# Intro

* Max Noichl, Utrecht University, doing a PhD in *theoretical Philosophy*
* Comp. humanities • model transfer • agent based models of epistemic communities
* Very WIP!
* Terminology will be a bit *sloppy*: E. g. "Embedding" has a rigurous definition in mathematics, and a relaxed one in ML.


# What's a latent space

* LLMs have internal 'hidden' representations, the [latent space.](#){.fragment .opens-modal
  data-modal-type="iframe"
  data-modal-url="https://poloclub.github.io/transformer-explainer/"}
* The **Manifold hypothesis**
* Manifold: A topological space that around each point resembles euclidean space.
* Manifold hypothesis: Most real, high dimensional data (e.g. images, texts) actually lies on low dimensional manifolds in high dim. space.


# The manifold hypothesis

::: {.r-stack}
![](images/Illustration_manifold_hypothesis.png){width="100%" fig-align="center"}
:::

# Manifold learning
* There are a number of techniques that make these latent structures accessible.
* UMAP, t-SNE, **Autoencoders**, tda mapper, DREAMS,  PACmap, STARmap, Isomap, PAGA,...

::: aside
@
:::

# Spatial metaphors 

* Many concepts we use  to describe our social world are drawn from spatial metaphors.
* [Spatial metaphors.](#){.fragment .opens-modal
  data-modal-type="image"
  data-modal-url="images/soc_sci_structures.png"}
* This is not a classification scheme!


# Social science in latent space?

* Can we extract these representations, 
* LLMs appear to have internal world models [@gurneeLanguageModelsRepresent2024a
](#){.fragment .opens-modal
  data-modal-type="image"
  data-modal-url="images/tegmark_world_model.png"}
* Can we extract 'social-world' models?


#  An illustration

* This is _not_ research, merely illustrative.
* Take tweets from the MdP's of the 19th Bundestag (German parliament, 2017-2021 (!)).
* 10-300 tweets per member.
* Embed using an LLM: Qwen 3 8B, prompted to focus on politics
* Run DREAMS on the resulting embedding. 


# 
<iframe
  src="images/bundestag_fullmap.html"
  width="800"
  height="800"
  style="border:0; display: block; margin: 0 auto;"
></iframe>

# 
<iframe
  src="images/bundestag_faceted.html"
  width="1300"
  height="1000"
  style="border:0; display: block; margin: 0 auto;"
></iframe>


# 
<iframe
  src="images/political_twitter_3d_umap.html"
  width="800"
  height="800"
  style="border:0; display: block; margin: 0 auto;"
></iframe>

# So what
* Amazing that this works
*

Zoo of manifold learning methods

UMAP, t-SNE, tda mapper, DREAMS,  PACmap, STARmap, Isomap, PAGA



# Other tasks depend on this working
* LLM'S in downstream data-labelling for comp. soc. sci./humanities tasks.
* "In-silico" social science. 


# So many problems!
* So many black-boxes:
    1. Pre-training-data
    2. LLM/RLHF
    3. Actual data
    4. Dimensionality Reduction.    
* How do we nudge the model into the right subspace? Prompts, sparse auto-encoders?



# But much to gain!
* Get the language in which we *talk about*, and *do* culture, aligned with empirical methods.
* Theoretical models (e. g. ABM's) could be run on these topologies.
* AI interpreatbility to ourselves.









<!-- 
# 

::: {.r-fit-text .highlight}
Second meeting
:::
::: {.r-fit-text }
Meeting on October 1, 13:00: Getting more concrete!
::: -->
