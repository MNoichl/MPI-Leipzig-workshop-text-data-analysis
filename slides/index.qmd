---
format: 
    revealjs:
        revealjs-url: reveal.js-5.2.1
        title-slide: false
        scroll: true
        pagetitle: "Data-Driven methods"
        theme: [default, custom.scss]
        navigation-mode: vertical
        revealjs-plugins: [multimodal]
        include-after-body:
        - file: includes/after-body.html
        multimodal:
           # overlaycolor: "rgba(0,0,0,.55)"
            overlaycolor: "rgb(255, 0, 0)"
            radius: "0"             
        include-in-header:
        - file: includes/in-header.html
        auto-stretch: false
        incremental: true 
        
bibliography: CompTemp.bib
# csl: the-journal-of-modern-history.csl
cite-method: citeproc
dependencies:
    - "custom.scss"
controls: true
width: 1400 # 1060
height: 1000 # 700
margin: 0.15 # 1
min-scale: 0.2
max-scale: 1.6

# background-image: "images/background_egg.png"
#quarto preview /Users/Noich001/Desktop/network_epistemology_playground/talk_BOCHUM/Data_and_ABMs.qmd
#quarto add martinomagnifico/quarto-multimodal
---
# {background-image="images/manifold_background_1.png" background-size="100%" background-position="left bottom" background-opacity="1."} 
::: {.absolute top=-50 left=700 background="#000" .title-block}
<h1 class="title">Social science \n in latent \n space</h1>
<p class="author"> Max Noichl</p>
<p class="date">2025-09-17</p>
:::


# 

<iframe
  src="qrcode.html"
  width="300"
  height="300"
  style="border:0; display: block; margin: 0 auto;"
></iframe>

Follow along at: [https://mnoichl.github.io/MPI-Leipzig-workshop-text-data-analysis](https://mnoichl.github.io/MPI-Leipzig-workshop-text-data-analysis)



# Intro & Disclaimers

* Max Noichl, Utrecht University, doing a PhD in *theoretical philosophy*
* Computational humanities • model transfer • agent based models of epistemic communities • Philosophy of ML & AI.
* Very WIP! – Theorizing an emergent phenomenon.
* Terminology will be *sloppy*: E. g. "Embedding" has a rigurous definition in mathematics, and a relaxed one in ML.


# What is a latent space?

* Artificial neural networks often have internal 'hidden' representations, which lie in a latent space. [E. g. Autoencoders](#){.fragment .opens-modal
  data-modal-type="image"
  data-modal-url="images/autoencoder.png"}
* LLMs have a latent space [as well.](#){.fragment .opens-modal
  data-modal-type="iframe"
  data-modal-url="https://poloclub.github.io/transformer-explainer/"}
  
  
# The Manifold hypothesis I
* **Manifold:** A topological space that around each point resembles euclidean space.
* **Manifold hypothesis:** Most real, high dimensional data (e.g. images, texts) actually lies on low dimensional manifolds in high dim. space. Data often has a low 'intrinsic dimensionality'.


# The manifold hypothesis II

::: {.r-stack}
![](images/Illustration_manifold_hypothesis.png){width="100%" fig-align="center"}
:::


# The manifold hypothesis III

* Epistemic status unclear (assumption, quasi-Kantian condition, empirical fact?)
* Why do we believe this? – Dimensionality reduction *works* (e.g. in prediction pipelines).

# Manifold learning

* There are a number of techniques that make these latent structures accessible.
* UMAP, t-SNE, Autoencoders, tda mapper, DREAMS,  PACmap, STARmap, Isomap, PAGA,...

<br>


::: aside
@mcinnesUMAPUniformManifold2018; @maatenVisualizingDataUsing2008; @bohmUnifyingPerspectiveNeighbor2020; @kuryDREAMSPreservingBoth2025 
:::

# Social science in latent space?

* Many concepts we use  to describe our social world are drawn from spatial metaphors.
* [Spatial metaphors.](#){.fragment .opens-modal
  data-modal-type="image"
  data-modal-url="images/soc_sci_structures.png"}
* This is not a classification scheme!
* Can we extract these representations?
* LLMs appear to have internal world models [@gurneeLanguageModelsRepresent2024a
](#){.fragment .opens-modal
  data-modal-type="image"
  data-modal-url="images/tegmark_world_model.png"}
* Can we extract 'social-world' models?


#  An illustration

* This is _not_ research, merely illustrative.
* Take tweets from the MdP's of the 19th Bundestag – German parliament, 2017-2021 
* 10-300 tweets per member.
* Embed using an LLM: Qwen 3 8B, prompted to focus on politics.
* Run UMAP on the resulting embedding. 

<br>

::: aside
@saltzerGermanFederalElection2021
:::

# 
<iframe
  src="images/bundestag_fullmap.html"
  width="800"
  height="800"
  style="border:0; display: block; margin: 0 auto;"
></iframe>

# 
<iframe
  src="images/bundestag_faceted.html"
  width="1300"
  height="1000"
  style="border:0; display: block; margin: 0 auto;"
></iframe>


# 
<iframe
  src="images/political_twitter_3d_umap.html"
  width="1000"
  height="800"
  style="border:0; display: block; margin: 0 auto;"
></iframe>


# So many problems!
* So many black-boxes:
    1. Pre-training-data
    2. LLM/RLHF
    3. Actual data
    4. Dimensionality Reduction.    
* How do we nudge the model into the right subspace? Prompts, sparse auto-encoders?
* Temporal analysis: Movement _in_ space vs. movement _of_ the space?

<br>

::: aside
@chariSpeciousArtSingleCell
:::

# Other tasks depend on this working
* LLM'S in downstream data-labelling for comp. soc. sci./humanities tasks.
* "In-silico" social science. @manningAutomatedSocialScience2024
* Why not cut out the the middle man? (Sampling)
* AI interpretability to ourselves.


# But this is promising!
* Get the language in which we *talk about*, and *do* culture, aligned with empirical methods.
* Theoretical models (e. g. ABM's) could be run on these topologies.


# Literature







<!-- 
# 

::: {.r-fit-text .highlight}
Second meeting
:::
::: {.r-fit-text }
Meeting on October 1, 13:00: Getting more concrete!


<!-- # {background-iframe="background_network.html" background-size="120%" background-position="left bottom" background-opacity="1."}
::: {.absolute top=-50 left=700 background="#000" .title-block}
<h1 class="title">Social science \n in latent \n space!</h1>
<p class="author"> Max Noichl</p>
<p class="date">2025-09-17</p>
::: -->


::: -->
