{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518ca145",
   "metadata": {},
   "source": [
    "# Text analysis workbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9148c08",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MNoichl/MPI-Leipzig-workshop-text-data-analysis/blob/main/workbook_01_text_analysis.ipynb) \n",
    "\n",
    "Welcome to google colab! Colab is a cloud-based notebook environment that allows you to write and execute code in the python programming language in the browser. It follows a notebook structure (like jupyter) in which you can write markdown text like this, as well as code in cells that can be executed.\n",
    "\n",
    "Below is one of these cells. You can run it either by clicking the little (▶️) button on the top left of the cell, or by clicking into it and then pressing shift+enter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cede5f6",
   "metadata": {},
   "source": [
    "If you want to continue working on this notebook, and make your own changes to the code, we'd reccomend you save your own copy, by clicking the \"File\" menu at the top left, and then \"Save a copy in Drive\". Please do this as it's easy to loose your work otherwise. You can then edit your own copy. You can also download it as an .ipynb file by clicking the \"File\" menu at the top left, \"Download\", and then \"Download .ipynb\". If you want to learn more about the functionalites of colab notebooks, we reccommend looking at this [basic colab features-notebook.](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
    "\n",
    "** Important:** Below we are going to run a LLM. This needs a more powerful computer, called an GPU. You can select this by doing Runtime -> Change Runtime Type -> T4 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039a9d1",
   "metadata": {},
   "source": [
    "# Part 1: Set-up\n",
    "At the beginning of this notebook, we need to set up all of the libraries/packages (reusable python-programs other people have written) that we are going to use during this session. For this we use a common python-package manager called 'pip'. Pip takes care of downloading the right versions, and installing them on our computer, which in this case is a server that's standing in a google-data-center, maybe in Belgium or Iowa. These installs will take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddebf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install pandas\n",
    "!pip install pyalex\n",
    "!pip install umap-learn\n",
    "!pip install datamapplot\n",
    "!pip install upgrade sentence-transformers \n",
    "!pip install seaborn\n",
    "!pip install genieclust\n",
    "!pip install litellm\n",
    "!pip install opinionated\n",
    "!pip install keybert\n",
    "!pip install keyphrase-vectorizers\n",
    "!pip install pystreamgraph\n",
    "\n",
    "!pip install -q --upgrade \"transformers>=4.45.2\" \"sentence-transformers>=3.0.1\"\n",
    "!pip install --upgrade 'nltk==3.8.1' \n",
    "!pip install pyzipper\n",
    "!pip install hdbscan\n",
    "\n",
    "!python3 -m spacy download en_core_web_sm # for English NER\n",
    "\n",
    "#%pip install -q --upgrade \"transformers==4.44.2\" \"sentence-transformers==2.7.0\" \"accelerate>=0.33\" \"keybert>=0.8.5\" \"torch>=2.2,<2.5\"\n",
    "\n",
    "\n",
    "\n",
    "# Check if utils directory exists, if not download from GitHub\n",
    "import os\n",
    "if not os.path.exists('utils'):\n",
    "    !wget -q https://raw.githubusercontent.com/MNoichl/data-driven-philosophy-GAP2025/main/utils/openalex_utils.py -P utils/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344e34a",
   "metadata": {},
   "source": [
    "# Part 2: Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746954f0",
   "metadata": {},
   "source": [
    "After setting up the packages, we need to import them. This makes the code in the packages available for us to use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload for development\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Core data science libraries\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import numpy as np   # Numerical computing and arrays\n",
    "import os           # Operating system interface\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt  # Basic plotting functionality\n",
    "import seaborn as sns           # Statistical data visualization\n",
    "from matplotlib.colors import rgb2hex\n",
    "\n",
    "# Academic data access\n",
    "import pyalex  # Interface to OpenAlex academic database\n",
    "\n",
    "# Dimensionality reduction and clustering\n",
    "import umap         # Uniform Manifold Approximation and Projection\n",
    "import datamapplot  # Interactive visualization for high-dimensional data\n",
    "\n",
    "# Natural language processing and AI\n",
    "import sentence_transformers  # Sentence embeddings using transformer models\n",
    "import litellm                # Unified interface for various LLM APIs\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Styling and aesthetics\n",
    "import opinionated  # Opinionated matplotlib styling\n",
    "plt.style.use('opinionated_rc')\n",
    "import colormaps as colormaps  # Extended colormap collection - https://pratiman-91.github.io/colormaps/\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# iframes:\n",
    "from IPython.display import IFrame\n",
    "\n",
    "\n",
    "from utils.openalex_utils import openalex_url_to_pyalex_query, process_records_to_df, get_records_from_dois, openalex_url_to_filename, download_openalex_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549c12e",
   "metadata": {},
   "source": [
    "# Part 3: Getting data\n",
    "\n",
    "In this notebook we are interested in analysing textual data. For this I have prepared a dataset of abstracts from articles published by researchers at the MPI. Alternatively, you can also look at the *SEP*, and I have set up code that allows you to quickly scrape your own dataset of abstracts from the *OpenAlex*-database. Finally, if you have your owns data-source to work with, I can show you how to hook it up to this notebook below. **Important:** You will only want to run one of the  sub-sections below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e8345",
   "metadata": {},
   "source": [
    "# Part 3.1 Abstracts from the MPI Leipzig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10129f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MPI abstracts CSV from GitHub (zipped to stay under GitHub's file size limit)\n",
    "mpi_csv_url = \"https://github.com/MNoichl/MPI-Leipzig-workshop-text-data-analysis/raw/refs/heads/main/files/MPI-abstracts-OA.csv.zip\"\n",
    "dataset_df = pd.read_csv(mpi_csv_url)\n",
    "\n",
    "text_data = list(dataset_df['text'])\n",
    "year_data = dataset_df['publication_year']\n",
    "title_data = dataset_df['title']\n",
    "\n",
    "used_dataset = \"MPI-Leipzig\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722cf2e",
   "metadata": {},
   "source": [
    "## Part 3.2 American Anthropologist-dataset\n",
    "\n",
    "You can also work with the early papers of the *American Anthropologist*, a dataset of some historical interest suited to concept analysis. Because it has strong copy-right restrictions, you will need to enter the secret key I provided via email below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_KEY = b\"...\"\n",
    "\n",
    "import requests\n",
    "import pyzipper\n",
    "\n",
    "zip_data = requests.get(\"https://github.com/MNoichl/MPI-Leipzig-workshop-text-data-analysis/raw/refs/heads/main/files/american_anthropologist_merged.zip\").content\n",
    "\n",
    "# Save the zip file\n",
    "zip_filename = \"american_anthropologist_merged.zip\"\n",
    "with open(zip_filename, 'wb') as f:\n",
    "    f.write(zip_data)\n",
    "\n",
    "with pyzipper.AESZipFile(zip_filename, mode=\"r\") as zf:\n",
    "    zf.setpassword(SECRET_KEY)\n",
    "    \n",
    "    # List files in the archive\n",
    "    file_list = zf.namelist()\n",
    "    print(f\"Files in encrypted zip: {file_list}\")\n",
    "    \n",
    "    # Extract the CSV file\n",
    "    csv_filename = file_list[0]  # Assuming there's only one file\n",
    "    with zf.open(csv_filename) as f:\n",
    "        df_decrypted = pd.read_csv(f, encoding='utf-8')\n",
    "\n",
    "print(f\"Successfully decrypted and loaded {len(df_decrypted)} records\")\n",
    "\n",
    "text_data = [x for x in df_decrypted['full_text']]\n",
    "year_data = [int(x[0:4]) for x in df_decrypted['published_date']]\n",
    "title_data = [x for x in df_decrypted['title']]\n",
    "\n",
    "used_dataset = \"American Anthropologist\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453be5f2",
   "metadata": {},
   "source": [
    "# Part 3.3 Stanford-Encyclopedia of philosophy dataset\n",
    "\n",
    "The code below downloads a zip-file of SEP-articles, and loads the texts into memory as a list, text_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2945543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import zipfile\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Download and extract if needed\n",
    "# stanford_zip = Path(\"stanford-encyclopedia.zip\")\n",
    "# stanford_folder = Path(\"stanford-encyclopedia\")\n",
    "# if not stanford_folder.exists():\n",
    "#     zip_data = requests.get(\"https://github.com/MNoichl/OZSW2025-data-driven-philosophy/raw/refs/heads/main/files/stanford-encyclopedia.zip\").content\n",
    "#     with zipfile.ZipFile(zipfile.io.BytesIO(zip_data)) as zip_ref:\n",
    "#         zip_ref.extractall(\".\")\n",
    "\n",
    "# # Load data\n",
    "# text_data = [open(f, encoding='utf-8').read() for f in stanford_folder.glob(\"*.txt\") if f.suffix in ['.txt', '.md']]\n",
    "# title_data = [f.stem for f in stanford_folder.glob(\"*\") if f.suffix in ['.txt', '.md']]\n",
    "# # Extract year data from copyright strings in the text files\n",
    "# import re\n",
    "\n",
    "# year_data = []\n",
    "# for text in text_data:\n",
    "#     # Look for \"Copyright © YYYY\" pattern at the beginning of the text\n",
    "#     match = re.search(r'Copyright © (\\d{4})', text)  \n",
    "#     if match:\n",
    "#         year_data.append(int(match.group(1)))\n",
    "#     else:\n",
    "#         year_data.append(None)  # If no copyright year found\n",
    "\n",
    "# used_dataset = \"Stanford Encyclopedia\"\n",
    "\n",
    "# print(len(text_data), len(title_data), len(year_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186423a",
   "metadata": {},
   "source": [
    "## Part 3.4 Other OpenAlex-datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c34791",
   "metadata": {},
   "source": [
    "I have written a function that takes in an arbitrary url to a OpenAlex search query, and downloads the abstracts associated with it. To use it, head over to [https://openalex.org](https://openalex.org), search for something you are interested in, and copy the web-address of your search address. Then replace the url behind ` openalex_url = `  with the new one. Make sure to keep the quotation marks around it. That tells python that this is a string of text, and not executable python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e54e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "openalex_url = 'https://openalex.org/works?filter=authorships.institutions.lineage:i4210118560,publication_year:2000-2025&page=1'\n",
    "\n",
    "dataset_df = download_openalex_records(openalex_url,\n",
    "                                       reduce_sample=True, \n",
    "                                       sample_reduction_method=\"All\", # \"All\", \"First n samples\", or \"n random samples\".\n",
    "                                       #sample_size=1000, \n",
    "                                       seed_value=\"42\")\n",
    "\n",
    "\n",
    "dataset_df['text'] = dataset_df['title'] + dataset_df['abstract'] \n",
    "# We filter for works that have an abstract:\n",
    "dataset_df = dataset_df[dataset_df['text'].str.len() > 40]\n",
    "\n",
    "text_data = list(dataset_df['text'])\n",
    "year_data = dataset_df['publication_year']\n",
    "title_data = dataset_df['title']\n",
    "\n",
    "used_dataset = \"OpenAlex-query\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f56043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1055fb16",
   "metadata": {},
   "source": [
    "## 3.5 Bring your own data\n",
    "\n",
    "Below you can hook up your own dataset. The remainder of the notebook expects `text_data`and  `title_data`to be lists of strings, and  `year_data`to be a list of numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b38033",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"...\"\n",
    "title_data = \"...\"\n",
    "year_data = \"...\"\n",
    "\n",
    "used_dataset = \"NAME YOUR DATASET HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5b837",
   "metadata": {},
   "source": [
    "# Part 4: Using a language model \n",
    "We are now going to use a text-embedding model (a relatively small large language model) to transform the texts into a format which is easier to analyze mathematically.  The most famous of these models is called BERT (Bidirectional encoder representations from transformers), but there are many models you can choose from for this purpose. Look around e. g. here: [https://www.sbert.net/docs/sentence_transformer/pretrained_models.html ](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html )\n",
    "\n",
    "While they are trained on different datasets and can have a variety of different architectures, their main principles are very similar: Sub-word-particles, called tokens, are associated with long strings of numbers, called embeddings. These embeddings learned during the training-process. When presented with new texts, as we are going to do below, the model cuts the text up into the tokens, and selects the associated embeddings. The embeddings then undergo a process called 'attention' in which the individual representations interact with, and change each-other. In this process, the model adapts the individual embeddings to their context, and changes e.g. the embeddings associated with 'bank', depending on whether the word is close to 'river', as opposed to 'deposit'. Finally, the embeddings of all the tokens in the text get summarized, e.g. by averaging thm. This final embedding, which contains information from all the tokens then gets used to represent the whole text.\n",
    "\n",
    "Here's a nice explanation: [https://poloclub.github.io/transformer-explainer/](https://poloclub.github.io/transformer-explainer/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f405f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src=\"https://poloclub.github.io/transformer-explainer/\", width=\"100%\", height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc184e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"all-mpnet-base-v2\" # \"answerdotai/ModernBERT-base\",  \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "model = SentenceTransformer(model_name) #thenlper/gte-small\n",
    "embeddings = model.encode([x[:3000] for x in text_data],\n",
    "                          show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4dcb4",
   "metadata": {},
   "source": [
    "This is what the resulting embeddings look like - each row represents one of our texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9d456",
   "metadata": {},
   "source": [
    "# Part 5: applying dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024105c3",
   "metadata": {},
   "source": [
    "The attractive thing about text embeddings is that we can calculate similarities between them, which will, due the way that the embedding model is trained, reflect the how similar the topics of the texts are. One way of analyzing these similarities is by using UMAP (Uniform Manifold approximation and Projection). What UMAP effectively does is compute a nearest-neighbour graph, in which each text is linked to the other texts that are most similar to it, reweigh the graph so that high density areas don't dominate, and then layout the resulting network in a lower dimensional space using a force based simulation, in which linked nodes are pulled together, while unconnected nodes are pushed apart. This reproduces important features of outr dataset, which originally where encoded in several hundred dimensions in two. For an intuitive explanation to umap, see: [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/). Most non-linear dimensionality-reduction methods are quite controversial. See this for guidelines on responisble use: [https://arxiv.org/abs/2508.15929](https://arxiv.org/abs/2508.15929). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src=\"https://pair-code.github.io/understanding-umap/\", width=\"100%\", height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=42,metric='cosine')\n",
    "umap_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "print(umap_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4a436",
   "metadata": {},
   "source": [
    "We can also look at the embeddings as a scatter-plot. Each data-point is one of our texts. Note that while the below looks like a scatter plot, the x- and y-axes hold no information. The plot is rotation-invariant, and only the distances between the datapoints are relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1],alpha=0.1, c='black')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf0ac6",
   "metadata": {},
   "source": [
    "# Part 6: Clustering \n",
    "\n",
    "To get a better idea of what the the different areas of the plot mean, we can employ clustering. Because the 2-d umap looses a lot of information (768 Dimensions just don't fit that well into 2), and most clustering-algorithms struggle with high dimensional data due to the so-caled *curse of dimensionality*, we run another UMAP-reduction to 30 dimensions, on which we cluster. Here we employ genieclust, a flexible version of agglomerative clustering([https://genieclust.gagolewski.com/weave/basics.html](https://genieclust.gagolewski.com/weave/basics.html)), that allows us to tune how imbalanced we allow cluster sizes to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee567b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=30, random_state=42,metric='cosine')\n",
    "umap_embeddings_high_dim = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genieclust\n",
    "using_hdbscan = False\n",
    "\n",
    "\n",
    "g = genieclust.Genie(n_clusters=15, gini_threshold=0.3)\n",
    "cluster_labels = g.fit_predict(umap_embeddings)\n",
    "print(cluster_labels[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1723451",
   "metadata": {},
   "source": [
    "## 6.1 Other clustering algorithms\n",
    "You can also experiment with either of the other algorithms below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b968ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "using_hdbscan = True\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, metric='euclidean',cluster_selection_epsilon=0.05)\n",
    "cluster_labels = clusterer.fit_predict(umap_embeddings_high_dim)\n",
    "print(f\"Number of clusters: {len(np.unique(cluster_labels_hdbscan))}\")\n",
    "print(f\"Number of noise points: {np.sum(cluster_labels_hdbscan == -1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain clustering using networkx\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Convert to networkx graph\n",
    "G = nx.from_scipy_sparse_array(reducer.graph_)\n",
    "\n",
    "# Run Louvain community detection\n",
    "communities = nx.community.louvain_communities(G, resolution=1., seed=42)\n",
    "\n",
    "# Convert communities to cluster labels\n",
    "cluster_labels = np.zeros(len(umap_embeddings_high_dim), dtype=int)\n",
    "for cluster_id, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        cluster_labels[node] = cluster_id\n",
    "\n",
    "print(f\"Number of clusters: {len(communities)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883facf1",
   "metadata": {},
   "source": [
    "# 6.2 Plot the clustering\n",
    "We now create a dictionary of colors, which we can use in all the later plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b50379",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = np.unique(cluster_labels)\n",
    "n_clusters = len(unique_clusters)\n",
    "cluster_centers = np.asarray([\n",
    "    umap_embeddings[cluster_labels == label].mean(axis=0)\n",
    "    for label in unique_clusters\n",
    "])\n",
    "palette = datamapplot.palette_handling.palette_from_cmap_and_datamap(\n",
    "    colormaps.hawaii,\n",
    "    umap_embeddings,\n",
    "    cluster_centers,\n",
    "    chroma_bounds=(30, 90),\n",
    "    lightness_bounds=(10, 70),\n",
    "    theta_range=np.pi / 16,\n",
    "    radius_weight_power=.9,\n",
    ")\n",
    "\n",
    "# If using HDBSCAN, replace first color (noise cluster -1) with grey\n",
    "if using_hdbscan and -1 in unique_clusters:\n",
    "    palette = list(palette)\n",
    "    noise_idx = np.where(unique_clusters == -1)[0][0]\n",
    "    palette[noise_idx] = '#808080'  # grey for noise points\n",
    "\n",
    "custom_color_map = dict(zip(unique_clusters, palette))\n",
    "repeated_colors = [custom_color_map[x] for x in cluster_labels]\n",
    "\n",
    "n_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d38b3d",
   "metadata": {},
   "source": [
    "And then we plot the umap from above with the clusters overlayed on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=umap_embeddings[:, 0], \n",
    "            y=umap_embeddings[:, 1],\n",
    "            c=[custom_color_map[x] for x in cluster_labels],\n",
    "            alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23adfb4d",
   "metadata": {},
   "source": [
    "# Part 7: Labeling clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4100ed9",
   "metadata": {},
   "source": [
    "We now want to find out what our clusters actually contain. One quick, and a little drity way to do this is to simpy show examples from each cluster to an LLM, and have it come up with a name for it. As we are using a strong LLM here via an API, we need to add an API-key in place of the \"...\" below. API keys are secrets that are directly connected to our credit-cards, so we have to treat them carefully! We are going to send the key for this workshop out via E-Mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ac189",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"...\"\n",
    "\n",
    "\n",
    "if API_KEY == \"...\": # This code below is only for local development. You can ignore it!\n",
    "    with open('API_KEYS.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('OPENAI:'):\n",
    "                API_KEY = line.split(':', 1)[1].strip()\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"OPENAI API key not found in API_KEYS.txt\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7aede",
   "metadata": {},
   "source": [
    "Now we can  create our labels. The `label_cluster`function takes in a list of text from each cluster, takes a random sample, and, using the prompt, asks the LLM to name them. Feel free to play with the prompt to steer the labels into the right direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "# Create a function to label clusters using OpenAI\n",
    "def label_cluster(cluster_texts, cluster_id, n_samples=10, extra_prompt=\"\"):\n",
    "    # Randomly sample representative texts from the cluster (max 10 for efficiency)\n",
    "    sample_size = min(10, len(cluster_texts))\n",
    "    sample_texts = random.sample(cluster_texts, sample_size)\n",
    "    \n",
    "    # Truncate texts to 100 characters\n",
    "    sample_texts = [text[:1000] + \"...\" if len(text) > 1000 else text for text in sample_texts]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Below are randomly sampled texts from cluster {cluster_id}. Please analyze these texts and provide:\n",
    "    1. A short descriptive label (2-4 words). \n",
    "    2. A brief description of the main theme.\n",
    "    {extra_prompt}\n",
    "\n",
    "    Texts:\n",
    "    {chr(10).join([f\"- {text[:200]}...\" if len(text) > 200 else f\"- {text}\" for text in sample_texts])}\n",
    "    \n",
    "    Please respond in JSON format:\n",
    "    {{\n",
    "        \"label\": \"[your label]\",\n",
    "        \"description\": \"[your description]\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"content\": prompt, \"role\": \"user\"}]\n",
    "    response = completion(model=\"openai/gpt-5\", messages=messages, response_format={\"type\": \"json_object\"})\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bde17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_prompt = \"Note that the clusters are areas of work from a scientific institution. Try to pick labels that are commonly used to describe research areas.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Group texts by cluster\n",
    "cluster_groups = {}\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    if label not in cluster_groups:\n",
    "        cluster_groups[label] = []\n",
    "    cluster_groups[label].append(text_data[i])\n",
    "\n",
    "# Label each cluster\n",
    "cluster_info = {}\n",
    "for cluster_id, texts in cluster_groups.items():\n",
    "    print(f\"Labeling cluster {cluster_id} ({len(texts)} texts)...\")\n",
    "    label_info = label_cluster(texts, cluster_id, n_samples=5, extra_prompt=additional_prompt)\n",
    "    cluster_info[cluster_id] = label_info\n",
    "    print(f\"Cluster {cluster_id}: {label_info}\\n\")\n",
    "\n",
    "# Create a summary dataframe\n",
    "cluster_summary = []\n",
    "for cluster_id, info in cluster_info.items():\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        parsed_info = json.loads(info)\n",
    "        label = parsed_info.get('label', 'Unknown')\n",
    "        description = parsed_info.get('description', 'No description available')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Fallback if JSON parsing fails\n",
    "        label = f\"Cluster {cluster_id}\"\n",
    "        description = str(info)\n",
    "    \n",
    "    if cluster_id == -1:\n",
    "        label = \"Noise\"\n",
    "        description = \"Noise points\"\n",
    "    \n",
    "    cluster_summary.append({\n",
    "        'Cluster_ID': cluster_id,\n",
    "        'Size': len(cluster_groups[cluster_id]),\n",
    "        'Label': label,\n",
    "        'Description': description\n",
    "    })\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_summary)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d23108",
   "metadata": {},
   "source": [
    "We also do some data-wrangling, to have one long list of labels for each data-point, and to update our label-dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a labels list from cluster_labels and the labeled cluster descriptions\n",
    "labels_list = []\n",
    "for cluster_id in cluster_labels:\n",
    "    # Find the corresponding label from the cluster_df\n",
    "    cluster_row = cluster_df[cluster_df['Cluster_ID'] == cluster_id]\n",
    "    label = cluster_row.iloc[0]['Label']\n",
    "    labels_list.append(label)\n",
    "\n",
    "# Update the custom_color_map to use cluster labels instead of cluster IDs\n",
    "custom_color_map = {}\n",
    "for cluster_id in unique_clusters:\n",
    "    # Find the corresponding label from the cluster_df\n",
    "    cluster_row = cluster_df[cluster_df['Cluster_ID'] == cluster_id]\n",
    "    label = cluster_row.iloc[0]['Label']\n",
    "    \n",
    "    # Get the color for this cluster_id from the original mapping\n",
    "    color_index = list(unique_clusters).index(cluster_id)\n",
    "    color = rgb2hex(repeated_colors[color_index])\n",
    "    custom_color_map[label] = color\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abe23b",
   "metadata": {},
   "source": [
    "# Part 8: Visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42298b",
   "metadata": {},
   "source": [
    "## 8.1 An interactive data-map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab29a29d",
   "metadata": {},
   "source": [
    "To bring everything we've done so far together, we produce an interactive map of our datast. This includes the umap, the clusters and the labels. On hovering over the datapoints we can see their titles. This allows us to explore the structure of our dataset, as well investigate properties, and potential failures of our data and pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamapplot\n",
    "\n",
    "plot = datamapplot.create_interactive_plot(\n",
    "    umap_embeddings,\n",
    "    labels_list,  \n",
    "    hover_text=[x.title() if x is not np.nan else \"\" for x in title_data],  \n",
    "    label_color_map=custom_color_map,\n",
    "    title=used_dataset\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5097ab",
   "metadata": {},
   "source": [
    "## 8.2 Historical developments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913ae64",
   "metadata": {},
   "source": [
    "One common application of topic-modelling/clustering is to investigate the historical development of a corpus. An easy way to accomplish this is a stream-graph, in which the size of each cluster changes as a function of time. We first have to do some data-wrangling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for streamgraph visualization\n",
    "# Convert the year_data and clusters into the format expected by plot_streamgraph\n",
    "\n",
    "# Get unique years and sort them\n",
    "years_sorted = sorted(set(year_data))\n",
    "X_stream = np.array(years_sorted)\n",
    "\n",
    "# Get unique clusters and their labels\n",
    "unique_clusters = sorted(set(cluster_labels))\n",
    "cluster_labels_to_plot = []\n",
    "for cluster_id in unique_clusters:\n",
    "    if cluster_id in cluster_df['Cluster_ID'].values:\n",
    "        label = cluster_df[cluster_df['Cluster_ID'] == cluster_id]['Label'].iloc[0]\n",
    "        cluster_labels_to_plot.append(label)\n",
    "    else:\n",
    "        cluster_labels_to_plot.append(f\"Cluster {cluster_id}\")\n",
    "\n",
    "# Create Y matrix: each row is a cluster's values over time\n",
    "Y_stream = []\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_values = []\n",
    "    for year in years_sorted:\n",
    "        # Count documents in this cluster for this year\n",
    "        cluster_count = sum(1 for i, (doc_year, doc_cluster) in enumerate(zip(year_data, cluster_labels)) \n",
    "                           if doc_year == year and doc_cluster == cluster_id)\n",
    "        cluster_values.append(cluster_count)\n",
    "    Y_stream.append(cluster_values)\n",
    "\n",
    "Y_stream = np.array(Y_stream)\n",
    "\n",
    "print(f\"Streamgraph data prepared:\")\n",
    "print(f\"X (years): {len(X_stream)} points from {X_stream[0]} to {X_stream[-1]}\")\n",
    "print(f\"Y (clusters): {Y_stream.shape[0]} clusters × {Y_stream.shape[1]} time points\")\n",
    "print(f\"Cluster labels: {cluster_labels_to_plot[:3]}...\")  # Show first 3 labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9144c0b",
   "metadata": {},
   "source": [
    "There is no good streamgraph-library in python that I know of, so I made my own and import it here. This version can sort the clusters by size over time, showing which clusters grow and fade proportionally, as well as the growth of the corpus as a whole in absolute numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740df1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystreamgraph import plot_streamgraph, order_bestfirst, order_twoopt\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "\n",
    "order = order_twoopt(X_stream, Y_stream, repeats=12, scans=4)\n",
    "\n",
    "\n",
    "X_stream = np.asarray(years_sorted, dtype=float)\n",
    "order = order_twoopt(X_stream, Y_stream, repeats=120, scans=40)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure path effects for better label readability\n",
    "stroke_effect = path_effects.withStroke(linewidth=2, foreground='white')\n",
    "bbox_style = dict(\n",
    "    boxstyle=\"round,pad=0.3\", \n",
    "    alpha=0., \n",
    "    edgecolor='none'\n",
    ") # We can add a bbox to get padding between the labels.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "\n",
    "ax = plot_streamgraph(\n",
    "                X_stream, \n",
    "                Y_stream, \n",
    "                labels=cluster_labels_to_plot,wiggle_reduction=\"l1_weighted\",#global_order=order, \n",
    "    cmap=custom_color_map,#colormaps.lipari.cut(0.25, 'right'),\n",
    "    \n",
    "    sorted_streams=False,  # Keep original stream order\n",
    "    label_position='balanced',  # Use annealed placement to avoid overlaps\n",
    "    label_balanced_inset_frac=0.02,  # Keep labels slightly inside stream boundaries\n",
    "    curve_samples=24,  # Smooth curve interpolation\n",
    "    smooth_window=10,\n",
    "    label_fontsize_min=6,  # Minimum font size for smaller streams\n",
    "    label_fontsize_max=18,  # Maximum font size for larger streams\n",
    "    label_color='black',\n",
    "    label_balanced_fit_tolerance_px=1.5,\n",
    "    label_balanced_T0=5,\n",
    "    label_balanced_T_min=5e-4,\n",
    "    label_balanced_alpha=0.94,\n",
    "    label_balanced_iters_per_T=500,\n",
    "    label_balanced_min_thickness_q=2,\n",
    "    label_kwargs={\n",
    "        'path_effects': [stroke_effect],  # White outline for better contrast\n",
    "        'bbox': bbox_style  # Semi-transparent white background box\n",
    "    },\n",
    "    label_anchor='middle_center',\n",
    "    #label_balanced_debug_segments=True,\n",
    "    ax=ax,\n",
    "    label_balanced_progress=True # Monitor the annealing progress of the label-placement.\n",
    ")\n",
    "\n",
    "plt.title(\"Streamgraph of the {} over time\".format(used_dataset))\n",
    "plt.gca().yaxis.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3bac5",
   "metadata": {},
   "source": [
    "# Getting embeddings for specific words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0121be",
   "metadata": {},
   "source": [
    "Instead of embedding the whole texts, we can also compute embeddings for individual tokens, or token-sequences. This allows us to trace the semantics of individual words, at least in so far as they are determined by their contexts (as understood by the llm).\n",
    "The two codeblocks first extract all the contexts of a keyphrase, and then extract the representations corresponding to the key-phrase from the embedding of the whole context. You can continue working witht the dataset from above, or download a new one aroudn a word that interests you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrase = \"evolution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb299b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openalex_url = f'https://openalex.org/works?page=1&filter=title_and_abstract.search:{key_phrase},has_abstract:true'\n",
    "\n",
    "dataset_df = download_openalex_records(openalex_url,\n",
    "                                       reduce_sample=True, \n",
    "                                       sample_reduction_method=\"n random samples\", # \"All\", \"First n samples\", or \"n random samples\".\n",
    "                                       sample_size=5000, \n",
    "                                       seed_value=\"42\")\n",
    "\n",
    "\n",
    "dataset_df['text'] = dataset_df['title'] + dataset_df['abstract'] \n",
    "# We filter for works that have an abstract:\n",
    "dataset_df = dataset_df[dataset_df['text'].str.len() > 40]\n",
    "\n",
    "text_data = list(dataset_df['text'])\n",
    "year_data = dataset_df['publication_year']\n",
    "title_data = dataset_df['title']\n",
    "\n",
    "used_dataset = \"OpenAlex-query\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc375a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_context_around_phrase(text_data, key_phrase, sentences_before=2, sentences_after=2):\n",
    "    \"\"\"\n",
    "    Extract text chunks around occurrences of a key phrase, including surrounding sentences.\n",
    "    \n",
    "    Parameters:\n",
    "    - text_data: list of strings or single string containing the text(s) to search\n",
    "    - key_phrase: string to search for\n",
    "    - sentences_before: number of sentences to include before the key phrase\n",
    "    - sentences_after: number of sentences to include after the key phrase\n",
    "    \n",
    "    Returns:\n",
    "    - list of dictionaries with 'text', 'doc_index', and 'phrase_position' keys\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Ensure text_data is a list\n",
    "    if isinstance(text_data, str):\n",
    "        text_data = [text_data]\n",
    "    \n",
    "    contexts = []\n",
    "    \n",
    "    for doc_idx, text in enumerate(text_data):\n",
    "        # Split text into sentences (simple approach using periods, exclamation marks, question marks)\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty sentences\n",
    "        \n",
    "        # Find sentences containing the key phrase\n",
    "        for sent_idx, sentence in enumerate(sentences):\n",
    "            if key_phrase.lower() in sentence.lower():\n",
    "                # Calculate context window\n",
    "                start_idx = max(0, sent_idx - sentences_before)\n",
    "                end_idx = min(len(sentences), sent_idx + sentences_after + 1)\n",
    "                \n",
    "                # Extract context sentences\n",
    "                context_sentences = sentences[start_idx:end_idx]\n",
    "                context_text = '. '.join(context_sentences)\n",
    "                \n",
    "                # Add period at the end if not present\n",
    "                if not context_text.endswith(('.', '!', '?')):\n",
    "                    context_text += '.'\n",
    "                \n",
    "                contexts.append({\n",
    "                    'text': context_text,\n",
    "                    'doc_index': doc_idx,\n",
    "                    'phrase_position': sent_idx,\n",
    "                    'sentence_range': (start_idx, end_idx-1)\n",
    "                })\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "# Extract contexts around the key phrase\n",
    "contexts = extract_context_around_phrase(text_data, key_phrase, sentences_before=2, sentences_after=2)\n",
    "context_texts = [context['text'] for context in contexts]\n",
    "\n",
    "# Let's also add the years to the contexts, for later:\n",
    "for x in contexts:\n",
    "    x['year'] = list(year_data)[x['doc_index']]\n",
    "    \n",
    "print(\"Found\", len(contexts), \"contexts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2151aa",
   "metadata": {},
   "source": [
    "We can check what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d122a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contexts[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d837d",
   "metadata": {},
   "source": [
    "Now we extract the embeddings for the key-phrase. This is a bit more work then above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "def phrase_vectors(key_phrase, texts, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    tok = model._first_module().tokenizer\n",
    "    needle = key_phrase.lower()\n",
    "    out = []\n",
    "\n",
    "    for text in tqdm(texts):\n",
    "        enc = tok(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "        offsets = enc[\"offset_mapping\"][0].tolist()  # (start_char, end_char) per token\n",
    "\n",
    "        with torch.no_grad():\n",
    "            token_embs = model.encode(text, output_value=\"token_embeddings\", convert_to_tensor=True, show_progress_bar=False)\n",
    "            # This is a bit inefficient, because we are not batching, but should be fine for most cases.\n",
    "\n",
    "        spans, i, hay = [], 0, text.lower() # Problematic for multiple KP's per chunk?\n",
    "        while True:\n",
    "            i = hay.find(needle, i)\n",
    "            if i == -1: break\n",
    "            spans.append((i, i + len(needle)))\n",
    "            i += 1\n",
    "\n",
    "        idxs = [t for t,(a,b) in enumerate(offsets)\n",
    "                if b > 0 and any(not (b <= s or a >= e) for s,e in spans)]\n",
    "\n",
    "        out.append(token_embs[idxs].mean(0).cpu() if idxs else None)\n",
    "\n",
    "    return out\n",
    "\n",
    "key_phrase_embeddings = np.array(phrase_vectors(key_phrase, context_texts, model_name=\"answerdotai/ModernBERT-base\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e11fc",
   "metadata": {},
   "source": [
    "We can again use umap for low dimensional, explorable embeddings, and genieclust to cluster. As we expect there to be one, or a few dominant meanings, we choose a higher target-genie-coeffiecient here (but ymmv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "\n",
    "key_phrase_reducer = umap.UMAP(n_components=2, random_state=42,metric='cosine')\n",
    "key_phrase_umap_embeddings = key_phrase_reducer.fit_transform(key_phrase_embeddings)\n",
    "\n",
    "print(key_phrase_umap_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genieclust\n",
    "\n",
    "key_phrase_high_dim_reducer = umap.UMAP(n_components=30, random_state=42,metric='cosine')\n",
    "key_phrase_high_dim_umap_embeddings = key_phrase_high_dim_reducer.fit_transform(key_phrase_embeddings)\n",
    "\n",
    "g = genieclust.Genie(n_clusters=3, gini_threshold=0.5)\n",
    "concept_cluster_labels = g.fit_predict(key_phrase_high_dim_umap_embeddings)\n",
    "\n",
    "\n",
    "n_clusters = len(np.unique(concept_cluster_labels))\n",
    "cb_colors = colormaps.tealrose(np.linspace(0, 1, colormaps.tealrose.N))\n",
    "repeated_concept_colors = np.tile(cb_colors, (n_clusters // colormaps.tealrose.N + 1, 1))[:n_clusters]\n",
    "\n",
    "custom_concept_color_map = dict(zip(np.unique(concept_cluster_labels),map(rgb2hex, repeated_concept_colors)))\n",
    "\n",
    "plt.scatter(key_phrase_umap_embeddings[:, 0], key_phrase_umap_embeddings[:, 1],alpha=0.8, c=[custom_concept_color_map[x] for x in concept_cluster_labels])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f86ff4",
   "metadata": {},
   "source": [
    "We can also reuse the labeling code from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a903ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group texts by concept cluster\n",
    "concept_cluster_groups = {}\n",
    "for i, label in enumerate(concept_cluster_labels):\n",
    "    if label not in concept_cluster_groups:\n",
    "        concept_cluster_groups[label] = []\n",
    "    concept_cluster_groups[label].append(contexts[i]['text'])\n",
    "\n",
    "# Label each concept cluster\n",
    "concept_cluster_info = {}\n",
    "for cluster_id, texts in concept_cluster_groups.items():\n",
    "    print(f\"Labeling concept cluster {cluster_id} ({len(texts)} texts)...\")\n",
    "    label_info = label_cluster(texts, cluster_id, n_samples=5, extra_prompt=f\"Note that these are clusters of text contexts containing the key phrase '{key_phrase}'. Try to pick labels that describe the different conceptual uses or meanings of this phrase.\")\n",
    "    concept_cluster_info[cluster_id] = label_info\n",
    "    print(f\"Concept cluster {cluster_id}: {label_info}\\n\")\n",
    "\n",
    "# Create a summary dataframe\n",
    "concept_cluster_summary = []\n",
    "for cluster_id, info in concept_cluster_info.items():\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        parsed_info = json.loads(info)\n",
    "        label = parsed_info.get('label', 'Unknown')\n",
    "        description = parsed_info.get('description', 'No description available')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Fallback if JSON parsing fails\n",
    "        label = f\"Concept Cluster {cluster_id}\"\n",
    "        description = str(info)\n",
    "    \n",
    "    concept_cluster_summary.append({\n",
    "        'Cluster_ID': cluster_id,\n",
    "        'Size': len(concept_cluster_groups[cluster_id]),\n",
    "        'Label': label,\n",
    "        'Description': description\n",
    "    })\n",
    "\n",
    "concept_cluster_df = pd.DataFrame(concept_cluster_summary)\n",
    "concept_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f08993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a labels list from concept_cluster_labels and the labeled concept cluster descriptions\n",
    "concept_labels_list = []\n",
    "for cluster_id in concept_cluster_labels:\n",
    "    # Find the corresponding label from the concept_cluster_df\n",
    "    cluster_row = concept_cluster_df[concept_cluster_df['Cluster_ID'] == cluster_id]\n",
    "    label = cluster_row.iloc[0]['Label']\n",
    "    concept_labels_list.append(label)\n",
    "\n",
    "# Update the custom_color_map to use concept cluster labels instead of cluster IDs\n",
    "concept_custom_color_map = {}\n",
    "unique_concept_clusters = sorted(set(concept_cluster_labels))\n",
    "for cluster_id in unique_concept_clusters:\n",
    "    # Find the corresponding label from the concept_cluster_df\n",
    "    cluster_row = concept_cluster_df[concept_cluster_df['Cluster_ID'] == cluster_id]\n",
    "    label = cluster_row.iloc[0]['Label']\n",
    "    \n",
    "    # Get the color for this cluster_id from the original mapping\n",
    "    color_index = list(unique_concept_clusters).index(cluster_id)\n",
    "    color = rgb2hex(repeated_concept_colors[color_index])\n",
    "    concept_custom_color_map[label] = color\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9ba9a",
   "metadata": {},
   "source": [
    "And build an interactive map of the meanings of our concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamapplot\n",
    "\n",
    "datamapplot.create_interactive_plot(\n",
    "    key_phrase_umap_embeddings,concept_labels_list,\n",
    "    hover_text=[text.replace(key_phrase, f\" █{key_phrase}█ \") for text in context_texts],  \n",
    "    label_color_map=concept_custom_color_map,\n",
    "    title=f\"{used_dataset} - {key_phrase}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EM_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
